model	val_acc	test_acc
albert-base-balanced_True-freeze_False	0.841	0.87
albert-base-balanced_True-freeze_True	0.615	0.6095
albert-large-balanced_True-freeze_False	0.893	0.909
albert-large-balanced_True-freeze_True	0.564	0.59
bert-base-balanced_True-freeze_False	0.85	0.866
bert-base-balanced_True-freeze_True	0.54	0.559
bert-large-balanced_True-freeze_False	0.927	0.9315
bert-large-balanced_True-freeze_True	0.53	0.516
electra-base-balanced_True-freeze_False	0.935	0.9255
electra-base-balanced_True-freeze_True	0.682	0.7025
electra-large-balanced_True-freeze_False	0.968	0.96
electra-large-balanced_True-freeze_True	0.693	0.726
electra-small-balanced_True-freeze_False	0.802	0.8085
electra-small-balanced_True-freeze_True	0.641	0.6395
meaning_matching-albert-base-n_neg10-balanced_True-freeze_False	0.862	0.873
meaning_matching-albert-base-n_neg10-balanced_True-freeze_True	0.54	0.5715
meaning_matching-albert-large-n_neg10-balanced_True-freeze_False	0.9	0.8965
meaning_matching-albert-large-n_neg10-balanced_True-freeze_True	0.566	0.573
meaning_matching-bert-base-n_neg10-balanced_True-freeze_False	0.879	0.8855
meaning_matching-bert-base-n_neg10-balanced_True-freeze_True	0.586	0.6005
meaning_matching-bert-base-n_neg5-balanced_True-freeze_False	0.878	0.885
meaning_matching-bert-base-n_neg5-balanced_True-freeze_True	0.559	0.5695
meaning_matching-bert-large-n_neg10-balanced_True-freeze_False	0.931	0.927
meaning_matching-bert-large-n_neg10-balanced_True-freeze_True	0.575	0.5975
meaning_matching-electra-base-n_neg10-balanced_True-freeze_False	0.931	0.936
meaning_matching-electra-base-n_neg10-balanced_True-freeze_True	0.717	0.738
meaning_matching-electra-large-n_neg10-balanced_True-freeze_False	0.963	0.9585
meaning_matching-electra-large-n_neg10-balanced_True-freeze_True	0.834	0.8285
meaning_matching-electra-small-n_neg10-balanced_True-freeze_False	0.812	0.8325
meaning_matching-electra-small-n_neg10-balanced_True-freeze_True	0.601	0.618
meaning_matching-roberta-base-n_neg10-balanced_True-freeze_False	0.885	0.8985
meaning_matching-roberta-base-n_neg10-balanced_True-freeze_True	0.756	0.76
meaning_matching-roberta-base-n_neg20-balanced_True-freeze_False	0.885	0.8955
meaning_matching-roberta-base-n_neg20-balanced_True-freeze_True	0.743	0.727
meaning_matching-roberta-base-n_neg3-balanced_True-freeze_False	0.881	0.8975
meaning_matching-roberta-base-n_neg3-balanced_True-freeze_True	0.724	0.729
meaning_matching-roberta-base-n_neg5-balanced_True-freeze_False	0.886	0.897
meaning_matching-roberta-base-n_neg5-balanced_True-freeze_True	0.747	0.762
meaning_matching-roberta-large-n_neg10-balanced_True-freeze_False	0.941	0.9445
meaning_matching-roberta-large-n_neg10-balanced_True-freeze_True	0.848	0.8635
roberta-base-balanced_True-freeze_False	0.893	0.8905
roberta-base-balanced_True-freeze_True	0.713	0.7065
roberta-large-balanced_True-freeze_False	0.94	0.945
roberta-large-balanced_True-freeze_True	0.703	0.7035
word_class_predict-albert-base-balanced_True-freeze_False	0.838	0.8485
word_class_predict-albert-base-balanced_True-freeze_True	0.598	0.6225
word_class_predict-albert-large-balanced_True-freeze_False	0.89	0.9055
word_class_predict-albert-large-balanced_True-freeze_True	0.615	0.5995
word_class_predict-electra-base-balanced_True-freeze_False	0.945	0.937
word_class_predict-electra-base-balanced_True-freeze_True	0.683	0.697
word_class_predict-electra-small-balanced_True-freeze_False	0.808	0.8315
word_class_predict-electra-small-balanced_True-freeze_True	0.569	0.604
word_class_predict-roberta-base-balanced_True-freeze_False	0.885	0.893
word_class_predict-roberta-base-balanced_True-freeze_True	0.684	0.7135
